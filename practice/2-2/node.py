import os
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from tavily import TavilyClient
from state import SearchState

load_dotenv()

llm = ChatOpenAI(
    model=os.getenv("LLM_MODEL_ID"),
    api_key=os.getenv("LLM_API_KEY"),
    base_url=os.getenv("LLM_BASE_URL"),
    temperature=0.7
)

tavily_client = TavilyClient(api_key=os.getenv("TAVILY_API_KEY"))

# --------------------- åˆ›å»ºèŠ‚ç‚¹ ---------------------

# ç†è§£å’ŒæŸ¥è¯¢èŠ‚ç‚¹
def understand_query_node(state: SearchState):
    """
    ç”¨äºç†è§£ç”¨æˆ·æ„å›¾ï¼Œå¹¶ä¸ºå…¶ç”Ÿæˆä¸€ä¸ªæœ€ä¼˜åŒ–çš„æŸ¥è¯¢
    """
    user_message = state["messages"][-1].content

    understand_prompt = f"""åˆ†æç”¨æˆ·çš„æŸ¥è¯¢ï¼š"{user_message}"
    è¯·å®Œæˆä¸¤ä¸ªä»»åŠ¡ï¼š
    1. ç®€æ´æ€»ç»“ç”¨æˆ·æƒ³è¦äº†è§£ä»€ä¹ˆ
    2. ç”Ÿæˆæœ€é€‚åˆæœç´¢å¼•æ“çš„å…³é”®è¯ï¼ˆä¸­è‹±æ–‡å‡å¯ï¼Œè¦ç²¾å‡†ï¼‰

    æ ¼å¼ï¼š
    ç†è§£ï¼š[ç”¨æˆ·éœ€æ±‚æ€»ç»“]
    æœç´¢è¯ï¼š[æœ€ä½³æœç´¢å…³é”®è¯]"""

    response = llm.invoke([SystemMessage(content=understand_prompt)])
    response_text = response.content

    # è§£æè¾“å‡º
    search_query = user_message     # é»˜è®¤å€¼ä¸ºç”¨æˆ·çš„åŸå§‹æŸ¥è¯¢
    if "æœç´¢è¯ï¼š" in response_text:
        search_query = response_text.split("æœç´¢è¯ï¼š")[1].strip()

    return {
        "user_query": response_text,
        "search_query": search_query,
        "step": "understood",
        "messages": [AIMessage(content=f"æˆ‘å°†ä¸ºæ‚¨æœç´¢ï¼š{search_query}")]
    }

# æœç´¢èŠ‚ç‚¹
def tavily_search_node(state: SearchState):
    """
    åŸºäºtavily apiè¿›è¡ŒçœŸå®æœç´¢ã€‚
    """
    search_query = state["search_query"]

    try:
        print(f"ğŸ” æ­£åœ¨æœç´¢ï¼š{search_query}")
        response = tavily_client.search(
            query=search_query,
            search_depth="basic",
            max_results=5,
            include_answer=True
        )

        search_results = ""

        # ä¼˜å…ˆä½¿ç”¨tavilyç”Ÿæˆçš„ç»¼åˆç­”æ¡ˆ
        if response.get("answer"):
            search_results = f"ç»¼åˆç­”æ¡ˆï¼š\n{response['answer']}\n\n"

        # æ·»åŠ å…·ä½“çš„æœç´¢ç»“æœ
        if response.get("results"):
            search_results += "ç›¸å…³ä¿¡æ¯ï¼š\n"
            for i, result in enumerate(response["results"][:3], 1):
                title = result.get("title", "")
                content = result.get("content", "")
                url = result.get("url", "")
                search_results += f"{i}. {title}\n{content}\næ¥æºï¼š{url}\n\n"

        if not search_results:
            search_results = "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        print(f"\næœç´¢ç»“æœå¦‚ä¸‹ï¼š\n{search_results}")

        return {
            "search_results": search_results,
            "step": "search_succeed",
            "messages": [AIMessage(content=f"âœ… æœç´¢å®Œæˆï¼æ­£åœ¨æ•´ç†ç­”æ¡ˆ...")]
        }

    except Exception as e:
        error_msg = f"æœç´¢æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(f"âŒ {error_msg}")

        return {
            "search_results": f"æœç´¢å¤±è´¥ï¼š{e}",
            "step": "search_failed",
            "messages": [AIMessage(content=f"âŒ æœç´¢é‡åˆ°é—®é¢˜...")]
        }

# å›ç­”èŠ‚ç‚¹
def generate_answer_node(state: SearchState):
    """
    åŸºäºæœç´¢ç»“æœç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚
    """
    if state["step"] == "search_failed":
        # é™çº§ä¸ºåŸºäºllmå·²æœ‰çŸ¥è¯†å›ç­”
        downgrade_prompt = f"æœç´¢apiä¸å¯ç”¨ï¼Œè¯·åŸºäºä½ æ‰€æœ‰çš„çŸ¥è¯†æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼š\nç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}"
        response = llm.invoke([SystemMessage(content=downgrade_prompt)])
    else:
        # å¦‚æœæœç´¢æˆåŠŸ
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœä¸ºç”¨æˆ·æä¾›å®Œæ•´ã€å‡†ç¡®çš„ç­”æ¡ˆï¼š
        ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}
        æœç´¢ç»“æœï¼š\n{state['search_results']}
        è¯·ç»¼åˆæœç´¢ç»“æœï¼Œæä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å›ç­”..."""

        response = llm.invoke([SystemMessage(content=prompt)])

    return {
        "final_answer": response.content,
        "step": "completed",
        "messages": [AIMessage(content=response.content)]
    }